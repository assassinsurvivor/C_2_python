{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "scrap_Data_anchal_windows.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "cEDANKYMsbmw",
        "colab_type": "code",
        "outputId": "cdead5b6-295d-4a80-843f-0a907f208920",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "!pip install selenium fuzzywuzzy\n",
        "#Imports\n",
        "import os\n",
        "import re\n",
        "import sys\n",
        "import nltk\n",
        "import spacy\n",
        "import sqlite3\n",
        "import selenium\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "from selenium import webdriver\n",
        "from fuzzywuzzy import fuzz\n",
        "from multiprocessing import Pool\n",
        "from IPython.display import display,HTML\n",
        "from sklearn.feature_extraction.text import *\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "\n",
        "#nlp = spacy.load(\"en_core_web_lg\")\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: selenium in /usr/local/lib/python3.6/dist-packages (3.141.0)\n",
            "Requirement already satisfied: fuzzywuzzy in /usr/local/lib/python3.6/dist-packages (0.17.0)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.6/dist-packages (from selenium) (1.24.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Z6eFffoCtanZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Setting chrome driver path\n",
        "CHROMEDRIVER_PATH=\"C:/driver/chromedriver.exe\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VtYzpPj8scM6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#creating db\n",
        "frame = 'dictionary_ms'\n",
        "sql_transaction = []\n",
        "\n",
        "def create_table():\n",
        "    connection = sqlite3.connect('{}.db'.format(frame))\n",
        "    c = connection.cursor()\n",
        "    c.execute(\"CREATE TABLE IF NOT EXISTS table_data(mcr_id INT PRIMARY KEY, data TEXT)\")\n",
        "    return connection,c\n",
        "\n",
        "def inject_data(cur_func,data_corpus):\n",
        "    sql_query = '''INSERT INTO table_data(mcr_id,data) VALUES(?,?) '''\n",
        "    cur_func.execute(sql_query, data_corpus)\n",
        "    print(cur_func.lastrowid)\n",
        "    return cur_func.lastrowid\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yONDDxnys1R4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def set_options_driver(headless=True,window_size=(1440,900)):\n",
        "    option_chrome=Options()\n",
        "    prefs = {\"translate\":{\"enabled\":\"true\"}}\n",
        "    option_chrome.add_experimental_option(\"prefs\", prefs)\n",
        "    option_chrome.headless=headless\n",
        "    driver=webdriver.Chrome(options=option_chrome,executable_path=CHROMEDRIVER_PATH)\n",
        "    driver.set_window_size(1440, 900) #If  windowsize is not set ,driver will give not able to interact.Error in headless mode\n",
        "    return driver\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def extract(addr=None):\n",
        "    #setting local variable ty to None because it would or wouldn't be populated based on scrapping results\n",
        "    ty=None \n",
        "    driver=set_options_driver(headless=True)\n",
        "    driver.get('https://www.google.com/')\n",
        "    driver.find_element_by_css_selector('input.gLFyf.gsfi').send_keys(addr)\n",
        "    driver.find_element_by_css_selector('input[name=btnK]').submit()\n",
        "    \n",
        "    try:\n",
        "        #If spelling correction is suggested by google api we click on it in order to correct the spelling\n",
        "        driver.find_element_by_css_selector('a.gL9Hy').click() \n",
        "    \n",
        "    except:\n",
        "        pass\n",
        "    \n",
        "    try:\n",
        "        \n",
        "        #Here we are trying to find the exact details shared on google maps \n",
        "        ty=driver.find_elements_by_css_selector('span.YhemCb')\n",
        "        ty=[i.text.replace('\\r', ' ').replace('\\n', ' ') for i in ty]\n",
        "        add=[i.text.replace('\\r', ' ').replace('\\n', ' ') for i in driver.find_elements_by_css_selector('span.ggV7z')]\n",
        "        ty+=add\n",
        "        \n",
        "       \n",
        "    except:\n",
        "        pass\n",
        "    \n",
        "    \n",
        "    headers=list(filter(lambda x:len(x)>0,[i.text.replace('\\r', ' ').replace('\\n', ' ') for  i in driver.find_elements_by_css_selector('h3.LC20lb')]))\n",
        "    headers=\" \".join(i for i in headers)\n",
        "    headers=list(filter(lambda x:len(x)>0,headers.split(\" \")))\n",
        "    return_head=headers\n",
        "    headers=headers*3\n",
        "    \n",
        "    if ty:\n",
        "        ty=ty*200 #giving additional weight \n",
        "        ty=\" \".join(i for i in ty)\n",
        "    else:\n",
        "        ty=\"\"\n",
        "\n",
        "\n",
        "    \n",
        "    \n",
        "    res=driver.find_elements_by_css_selector('div.r')\n",
        "    res=[i.find_elements_by_tag_name('a') for i in res]\n",
        "    res=[i[0].get_attribute('href') for i in res]\n",
        "    \n",
        "    links=list(filter(lambda x:len(x)!=0,res))\n",
        "    \n",
        "    span_el=driver.find_elements_by_css_selector('span.st')\n",
        "    front_corpus=[i.text.replace('\\r', ' ').replace('\\n', ' ') for i in span_el]\n",
        "    texter=[i.strip() for i in front_corpus]\n",
        "    pattern=re.compile(r\"[^a-zA-Z-&']\")\n",
        "    subheads= \" \".join(i for i in [\" \".join(i for i in re.sub(pattern,\" \",i).split(\" \") if len(i)!=0) for i in texter])\n",
        "    \n",
        "\n",
        "\n",
        "    corpus=[]\n",
        "    list_el=[]\n",
        "    list_yl=[]\n",
        "    for ii in links:\n",
        "        #print(ii)\n",
        "        \n",
        "        \n",
        "        corpuss=[]\n",
        "        try:\n",
        "            \n",
        "            driver.get(ii)\n",
        "            \n",
        "            try:\n",
        "                list_el+=[driver.find_element_by_css_selector('span.category-str-list').text.replace('\\r', ' ').replace('\\n', ' ')]*10\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "\n",
        "\n",
        "            try:\n",
        "                list_yl+=[driver.find_element_by_css_selector('dd.categories').text.replace('\\r', ' ').replace('\\n', ' ')]*10\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "\n",
        "            temp_cache=driver.find_element_by_tag_name(\"body\").text.replace('\\r', ' ').replace('\\n', ' ')\n",
        "            html=driver.page_source\n",
        "            soup=BeautifulSoup(html)\n",
        "            \n",
        "            results=soup.findAll('p')\n",
        "            for j in results:\n",
        "                corpuss+=(j.text.replace('\\r', ' ').replace('\\n', ' ').split()) #words+=j.text.strip().split(\" \")\n",
        "            corpuss+=temp_cache.split()\n",
        "            \n",
        "            corpus+=corpuss\n",
        "            #print(len(corpus))\n",
        "\n",
        "        except:\n",
        "            print(\"Exec--> \",i)\n",
        "            pass\n",
        "    driver.close()\n",
        "    \n",
        "    #removing trailing spaces and new line characters\n",
        "    corpus=[i for i in corpus]\n",
        "    #print(\"final\",len(corpus))\n",
        "    \n",
        "    #Any word having length less than 3 would be removed from the corpus\n",
        "    #corpus=list(filter(lambda x: len(x)>2,corpus))[:100000]\n",
        "    list_el=\" \".join(i for i in list_el)\n",
        "    list_yl=\" \".join(i for i in list_yl)\n",
        "    \n",
        "  \n",
        "\n",
        "    #All the information scrapped from different  links is merged into one para\n",
        "    \n",
        "    \n",
        "    corpus=subheads+\" \"+ty+\" \"+list_el+\" \"+list_yl+\" \"+\" \".join(i for i in corpus)\n",
        "    \n",
        "    \n",
        "    \n",
        "   \n",
        "    return corpus\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XJITglPFtEb0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "list=[\"svm\",\"knn\",\"random forest\",\"xgboost\",\"light gbm\",\"cnn\",\"rnn\",\"lstm\",\"attention lstm\",\"object detection\",\"ssd\",\"faster rcnn\"]\n",
        "\n",
        "for idx,i in enumerate(list):\n",
        "      \n",
        "    corpus=extract(i)\n",
        "    sql_transaction.append([idx,corpus])\n",
        "    \n",
        "        \n",
        "  if len(sql_transaction)==5:\n",
        "      connector,cur=create_table()\n",
        "      with connector:\n",
        "          for i in sql_transaction:\n",
        "              inject_data(cur,i)\n",
        "          sql_transaction=[]\n",
        "          connector.commit()\n",
        "\n",
        "\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3K2-rlIBtLF1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}