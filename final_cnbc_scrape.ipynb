{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import urllib.request\n",
    "import requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# extract pages \n",
    "\n",
    "# this is input url\n",
    "html_page = requests.get('https://www.cnbc.com/hospitals/').text\n",
    "\n",
    "soup = BeautifulSoup(html_page, \"lxml\")\n",
    "for link in soup.findAll('a'):\n",
    "    links_list.append(link.get('href'))\n",
    "\n",
    "#sel_links_list = [i for i in links_list if ((\"html\" in str(i) or \"page\" in str(i)) and (\"video\" not in str(i)))]\n",
    "last_page = [i for i in links_list if (\"?page\" in str(i))]\n",
    "last_page_num = int(re.findall(r'\\d+',last_page[len(last_page)-1])[0])\n",
    "last_page_num\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# extract all pages linked to url\n",
    "# extract all articles in each page of url\n",
    "# saves article, url, author, headline, posttime in json for each article\n",
    "\n",
    "for page_num in range(1,last_page_num+1):\n",
    "    \n",
    "    url_path = 'https://www.cnbc.com/hospitals/'+ '?page' + str(page_num)\n",
    "    links_list = []\n",
    "    html_page = requests.get('https://www.cnbc.com/hospitals/').text\n",
    "    soup = BeautifulSoup(html_page, \"lxml\")\n",
    "    for link in soup.findAll('a'):\n",
    "        links_list.append(link.get('href'))\n",
    "\n",
    "    sel_links_list = [i for i in links_list if ((\"html\" in str(i)) and (\"video\" not in str(i)))]\n",
    "    for link_num in range(len(sel_links_list)):\n",
    "        article_url = 'https://www.cnbc.com' + str(sel_links_list[link_num])\n",
    "        page = urllib.request.urlopen(article_url)\n",
    "        soup = BeautifulSoup(page, 'html.parser')\n",
    "        \n",
    "        headline_soup = soup.findAll('h1', attrs={'class': 'ArticleHeader-headline'})\n",
    "        headline_soup = re.sub('<.*?>', '', str(headline_soup))\n",
    "        headline = str.replace(headline_soup, \"\\n\",\"\")\n",
    "\n",
    "        author_soup = soup.findAll('div', attrs={'class': 'Author-authorNameAndSocial'})\n",
    "        author_soup = re.sub('<.*?>', '', str(author_soup))\n",
    "        author = str.replace(author_soup, \"\\n\",\"\")\n",
    "\n",
    "        time_soup = soup.findAll('div', attrs={'class': 'ArticleHeader-time'})\n",
    "        time_soup = re.sub('<.*?>', '', str(time_soup))\n",
    "        time = str.replace(time_soup, \"\\n\",\"\")\n",
    "        \n",
    "        \n",
    "        text_soup = soup.findAll('div', attrs={'class': 'group'})\n",
    "        text_soup = re.sub('<.*?>', '', str(text_soup))\n",
    "        text = str.replace(text_soup, \"\\n\",\"\")\n",
    "        \n",
    "        save_dataframe = pd.DataFrame({'article_url' : article_url,\n",
    "                      'headline' : headline,\n",
    "                      'author' : author,\n",
    "                      'time' : time,\n",
    "                      'article' : text},index=[0])                                        \n",
    "                                                  \n",
    "        save_path = \"C:/Users/ak18474/Jupyter notebooks/Web scraping/\" + \"page_\" + str(page_num)+ \"_file_\" + str(link_num) + \".json\"\n",
    "        save_dataframe.to_json(save_path)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
